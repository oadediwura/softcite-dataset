---
title: "Find quotes in PDF text"
output: html_notebook
---

```{r}
# library(plyr)
library(tidyverse)
library(data.world) # loads saved config see quickstart vignette

prefixes <- "
PREFIX bioj: <http://james.howison.name/ontologies/bio-journal-sample#>
PREFIX bioj-cited: <http://james.howison.name/ontologies/bio-journal-sample-citation#>
PREFIX ca: <http://floss.syr.edu/ontologies/2008/4/contentAnalysis.owl#>
PREFIX citec: <http://james.howison.name/ontologies/software-citation-coding#> 
PREFIX dc: <http://dublincore.org/documents/2012/06/14/dcmi-terms/>
PREFIX doap: <http://usefulinc.com/ns/doap#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX vivo: <http://vivoweb.org/ontology/core#>
PREFIX xml: <http://www.w3.org/XML/1998/namespace>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
"

softcite_ds = "https://data.world/jameshowison/software-citations/"

# should pull from coding scheme
valid_codes = c("has_supplement",
"has_in_text_mention",
"coded_no_in_text_mentions",
"memo",
"full_quote",
"on_pdf_page",
"spans_pages",
"mention_type",
"software_was_used",
"software_name",
"version_number",
"version_date",
"url",
"creator",
"has_reference",
"reference_type")
```

This gets the codes from the top of the files.

```{r}
top_code_query <- data.world::qry_sparql(paste(prefixes,
      "SELECT ?article ?coder ?selection ?full_quote ?on_pdf_page ?spans_pages
WHERE {
    ?article citec:has_in_text_mention ?selection .
    ?selection ca:isTargetOf
        [ rdf:type ca:CodeApplication ;
          ca:hasCoder ?coder ;
          ca:appliesCode [ rdf:type citec:mention_type ]
        ] .
    ?selection citec:full_quote ?full_quote ;
               citec:on_pdf_page ?on_pdf_page ;
               citec:spans_pages ?spans_pages
    }"
))

top_codes <- data.world::query(top_code_query, softcite_ds)
top_codes <- as.tibble(top_codes) %>% 
<<<<<<< HEAD
  filter(str_detect(article, "PMC")) %>% 
  filter(!str_detect(article, "_"))

top_codes %>% group_by(article) %>% summarize(num_coder = n_distinct(coder)) %>% group_by(num_coder) %>% tally()
# write_csv(top_codes, "top_codes.csv")
=======
  filter(str_detect(article, "PMC"))

# top_codes %>%
#   group_by(article) %>%
#   summarize(num_coder = n_distinct(coder)) %>%
#   group_by(num_coder) %>%
#   tally()
```

```{r}
no_selection_query <- data.world::qry_sparql(paste(prefixes,
      "SELECT ?article ?coder 
       WHERE { ?article ca:isTargetOf
               [ rdf:type ca:CodeApplication ;
                 ca:hasCoder ?coder ;
                 ca:appliesCode [ rdf:type citec:coded_no_in_text_mentions ;
                                  citec:isPresent true ]
               ]
    }"
))

no_selection_articles <- data.world::query(no_selection_query, 
                                           dataset = softcite_ds) %>% 
  mutate(article = str_extract(article, "[#/]([^#/]+)$"),
         article = str_sub(article,2),
         matched = 0,
         unmatched = 0) %>% 
  collect()
>>>>>>> 2c405452a0bc214db2014bbb971e97da45d139cc
```


```{r}
# top_codes <- read_csv("top_codes.csv")
```


SPARQL queries return everything as URLs, but we want just the localPart at the end.

```{r}
top_codes <- top_codes %>%
       mutate_at(vars(article, selection),
                 funs(str_extract(.,"[#/]([^#/]+)$"))) %>%
       mutate_at(vars(article,selection), funs(str_sub(.,2)))
```


```{r}
# Some summary stats.

top_codes %>% summarize(num_articles = n_distinct(article),
                        num_coders = n_distinct(coder),
                        num_selections = n_distinct(selection))

top_codes %>% group_by(article, coder) %>% 
  summarize(num_selections = n_distinct(selection)) %>% 
<<<<<<< HEAD
  arrange(desc(num_selections))
=======
  arrange(desc(num_selections)) %>% 
  filter(article == "PMC4441238")
>>>>>>> 2c405452a0bc214db2014bbb971e97da45d139cc

```

```{r}
norm_text <- function(text_to_norm){
  text_to_norm %>% 
    str_replace_all("[\\r\\n\\s]" , "") %>% 
    str_replace_all("\\(http.*?\\)" , "") %>% 
    str_to_lower()
}

test_txt = "The cat sat ON the (http://REMOVE_ME)"
norm_text(test_txt)
```


Try locating in XML files.

```{r}
library(xml2)
library(glue)

<<<<<<< HEAD
path = "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/docs/pdf-files/pmc_oa_files/"
=======
path = "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-pdf-files/docs/pdf-files/pmc_oa_files/"
>>>>>>> 2c405452a0bc214db2014bbb971e97da45d139cc

test_articles <- tribble(
  ~pmcid,
  "PMC5381613", 
  "PMC5421183" 
)

articles <- top_codes %>% select(article) %>% distinct()

articles <- articles %>% 
  mutate(xml_path       = glue_data(., "{path}{article}.xml"),
         article_as_xml = map(xml_path, read_xml),
         pdf_as_string  = map_chr(article_as_xml, xml_text),
         pdf_as_string  = norm_text(pdf_as_string))
                        
```

Now load the pdf for each article

```{r, eval=F}
# library(extractr)
# 
# # locate PDFs
# folder <-  "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/docs/pdf-files/pmc_oa_files/"
# 
# articles <- top_codes %>% 
#   select(article) %>% 
#   distinct() %>% 
#   mutate(path = str_c(folder, article, ".pdf", sep=""))
# 
# # load pdfs.
# my_extract <- function(p) {
#   pdf_t <- extract(p)
#   as_text <- pdf_t$data
#   no_linebreaks <- str_replace_all(as_text, "[\\r\\n\\s]" , "")
#   return(no_linebreaks)
# }
# 
# # with an unvectorized function use map (or use rowwise())
# articles <- articles %>% 
#   mutate(pdf_as_string = map_chr(path, my_extract)) 

# articles %>% filter(article == "PMC1747177")

# known_problematic = c("PMC5039120_SK02", "PMC5039120_BB02",
#                       # interspersed columns
#                       "PMC3198286_MD03", 
#                       # interspersed headers
#                       "PMC4926940_MS01", "PMC4926940_SK01",
#                       # interspersed columns
#                       "PMC5039120_BB01", "PMC5039120_SK01",
#                       # interspersed columns
#                       "PMC5238813_RA08", "PMC5238813_SK28",
#                       # article has extraction issues, these two don't match
#                       "PMC5080194_MD02", "PMC5080194_RA01",
#                       # below here are those that find two matches
#                       # These two are found twice in the same article)
#                       "PMC5339831_BB01", "PMC5339831_SK01",
#                       # These two match but are same text as next two
#                       "PMC5339831_BB02", "PMC5339831_SK02",
#                       # These two match but are same as last two.
#                       "PMC3309518_RA09")
#                       # This one shows up twice.

```

Do matching using Biostrings.

```{r}
library(Biostrings)
library(tidyverse)
library(clipr)

top_codes <- top_codes %>% 
  mutate(norm_full_quote = norm_text(full_quote))

# for each article, run a function
match_in_article <- function(df) {
  #df <- top_codes[36, ]
  art <- df[[1, "article"]] # grouping variable
  pdf_as_string = (articles %>% filter(article == art))$pdf_as_string[[1]] 
  # ddply(df, .(selection), match_selection, BString(pdf_as_string))
  df %>% group_by(selection) %>% do(match_selection(., BString(pdf_as_string)))
}

match_selection <- function(df, pdf_as_xstring) {
  norm_full_quote <- df[[1, "norm_full_quote"]]
  # Attempt exact match (without indels)
  results <- matchPattern(norm_full_quote, pdf_as_xstring)
  if (length(results) == 0) {
      mismatch_allow <- floor(str_length(norm_full_quote) / 10)
      # mismatch_allow <- 10
      results <- matchPattern(norm_full_quote, pdf_as_xstring,
                          with.indels = T, max.mismatch = mismatch_allow)
  }
  
  if (length(results) == 0) {
    # warning(paste0("Not found: ", df[[1, "selection"]], "\n"))
    df %>% mutate(found = F, num_found = 0, start_val = NA, end_val = NA)
  } else {
    df %>% mutate(found    = T,
                 num_found = length(results),
                 start_val = list(start(results)),
                 end_val = list(end(results)))
  }
}

# the dot here is a pronoun for the group, ie the split up dataframe.
(found <- top_codes %>% 
  # filter(article == "PMC5042035") %>% # all match
  # filter(article == "PMC5390613") %>%  # some misses 
  # filter(article == "PMC2518490") %>% # | article == "PMC2978351") %>% 
  group_by(article) %>% 
  do(match_in_article(.)) %>% # adds the num_found, start_val, end_val
  filter(num_found == 1) %>% 
  unnest() %>% # unlists the start_val and end_val columns
  ungroup() %>% 
  select(-norm_full_quote, -num_found, -found, -spans_pages) %>% 
  collect())

# Now need to figure out which overlap with which.

# Selections can only match selections in the same article. 
# Can only match selections by another coder as well.
# Can identify matches then drop those by self (which would include matching own range)

# chain broken to allow joining back after matching

(matching <- found %>% 
  # dots below mean "data frame as passed in" it's a self join.
  inner_join(x = ., y = ., by = "article") %>% 
  # can't match own selections
  filter(coder.x != coder.y) %>%
  # overlap definition
  # from http://wiki.c2.com/?TestIfDateRangesOverlap
  # ( start1 <= end2 and start2 <= end1 )
  filter(start_val.x <= end_val.y, 
          start_val.y <= end_val.x) %>% 
  # used as a check
  # select(full_quote.x, full_quote.y) %>% 
  # unite(matched_selections, selection.x, selection.y, sep="---") %>% 
  # mutate(matched_selections = ( matched_selections %>% 
  #                                str_split("---") %>% 
  #                                str_sort() %>% 
  #                                str_join() ) ) %>%  
  select(selection = selection.x, 
         matching_selection = selection.y) %>%
 #  distinct(selection, matching_selection, keep_all=T) %>% 
  # rowid_to_column() %>% 
  # unite(matched_id, article, rowid) %>% 
  collect())

# Note that matching is doubled each way. Probably an issue.
# very likely need to create an "id" or at least unique the matched ones.
# Hmmm, the relationship is more complex, since one large quote by one coder
# could match many by the others.  Oh well, can't deal with everything.

# left join back to found to leave NAs for those that didn't match  
(found_with_matches <- found %>% 
  left_join(matching, by = "selection") %>% 
  mutate(matched = if_else(is.na(matching_selection), "unmatched", "matched")) %>% 
  collect()) # dummy op to allow commenting out steps.

# found %>% group_by(coder) %>% 
#   summarize(num_selections = n_distinct(selection)) %>% 
#   arrange(desc(num_selections))
<<<<<<< HEAD
```



```{r}


# summarize situation.
(simple_agree_counts_by_article <- found_with_matches %>%
  group_by(article, matched) %>% 
  summarize(sel_count  = n_distinct(selection),
            num_coders = n_distinct(coder)) %>% 
  spread(matched, sel_count, fill=0) %>% 
  mutate(matched_number   = (matched / 2), # adjust for A match B, B match A
         total_selections = matched_number + unmatched,
         agree_percent = (matched_number / total_selections) %>% round(2)) %>% 
  arrange(desc(agree_percent)) %>% 
  write_csv("simple_agree_counts_by_article.csv") %>% 
  collect())

View(simple_agree_counts_by_article)

simple_agree_counts_by_article %>% 
  ggplot() +
  geom_histogram(aes(x=agree_percent), bins=10) +
  facet_wrap(~num_coders)
```

```{r}
(simple_agree_counts_by_article %>% 
  ungroup() %>% 
  filter(num_coders > 1) %>% 
  summarize(total_selections = sum(total_selections),
            total_match = sum(matched_number),
            overall_agree = (total_match / total_selections) %>% round(2)) %>% 
  collect())
```

# per coder is a little more problematic; and only relevant on articles with 2
# coders.
(agree_counts_per_coder <- found_with_matches %>% 
  group_by(article) %>% 
  mutate(num_coders_for_article = n_distinct(coder)) %>% 
  filter(num_coders_for_article > 1) %>% 
  ungroup() %>% 
  group_by(coder, matched) %>% 
  summarize(sel_count  = n_distinct(selection),
            num_coders = n_distinct(coder)) %>% 
  spread(matched, sel_count, fill=0) %>% 
  mutate(matched_number   = (matched / 2), # adjust for A match B, B match A
         total_selections = matched_number + unmatched,
         agree_percent = (matched_number / total_selections) %>% round(2)) %>% 
  collect())

View(agree_counts_per_coder)

# PMC2518490_MS01 matches PMC2518490_MD04
```


```{r}
found %>% filter(coder == "mrcynds")
```


```{r}


found

View(found)

found %>% 
  group_by(article) %>%
  mutate(matches = interval_overlap(interval, interval))

# for each article, split by coder then findOverlaps 
found %>% ungroup() %>% select(start, end) %>% as.matrix()

# get_matches_in_article <- function(df_article) {
#   # df_article <- found %>% ungroup()
#   starts <- df_article %>% pull(start)
#   ends <- df_article %>% pull(end)
#   intervals <- Intervals(as.matrix(data.frame(starts, ends)))
#   interval_overlap(intervals, intervals)
# }

get_matches_in_article <- function(s, e) {
  intervals <- Intervals(as.matrix(data.frame(s, e)))
  interval_overlap(intervals, intervals)
}

found %>% 
  group_by(article) %>%
  do(matches_for_article = get_matches_in_article(start_val, end_val))
         
# Probable issues in passing in found, which is grouped.
get_possible_matches <- function(df, found) {
  # filter the found dataframe to find possible matches
  curr_article <- df[[1,"article"]]
  curr_coder <- df[[1,"coder"]]
  others <- found %>% 
    filter(article == curr_article, coder != curr_coder ) %>% 
    transmute(possible_match = selection, 
              possible_range = range)
  
  df %>% rowwise() %>% mutate(poss_match = list(others))
}

# return_group <- function(df) {
#   # head(df, 2)
#   m = tibble(new_col = "hat")
#   m$selection <- df$selection
#   m
# }
# 
# found %>% group_by(article) %>% do(return_group(.))

poss_matches <- found %>%
  select(article, coder, selection, range) %>% 
  group_by(article, coder) %>%
  do(get_possible_matches(., found)) %>% 
  unnest(poss_match)

# poss_matches %>% unnest(poss_match)

# poss_matches[[300,"range"]]

# Now have a row for every possible pair.
# Must have found available
does_overlap <- function(sel_range, poss_overlap_range) {
 # sel_range_list <- (found %>% filter(selection == sel))[[1, "range"]]
#  sel_range <- sel_range_list[[1]] # unlist
#  poss_overlap_range_list <- found %>% 
#  filter(selection == poss_overlap) %>% pull(range)
 # poss_overlap_range <- poss_overlap_range_list[[1]] # unlist
  num_overlap = nrow(as.matrix(findOverlaps(sel_range, poss_overlap_range)))
  return(num_overlap != 0)
}
=======
```



```{r}
>>>>>>> 2c405452a0bc214db2014bbb971e97da45d139cc


<<<<<<< HEAD
poss_matches <- poss_matches %>% 
  mutate(match = map2(range, possible_range, does_overlap))

poss_matches %>% group_by(match) %>% tally()

final_matches <- poss_matches %>% filter(match == T) %>% 
  rename(matching_selection = poss_match) %>% 
  select(selection, matching_selection)
=======
# summarize situation.
# (simple_agree_counts_by_article <- found_with_matches %>%
#   group_by(article, matched) %>% 
#   summarize(sel_count  = n_distinct(selection),
#             num_coders = n_distinct(coder)) %>% 
#   spread(matched, sel_count, fill=0) %>% 
#   mutate(matched_number   = (matched / 2), # adjust for A match B, B match A
#          total_selections = matched_number + unmatched,
#          agree_percent = (matched_number / total_selections) %>% round(2)) %>% 
#   arrange(desc(agree_percent)) %>% 
#   write_csv("simple_agree_counts_by_article.csv") %>% 
#   collect())


(match_counts <- found_with_matches %>%
  group_by(article, coder, matched) %>% 
  summarize(sel_count  = n_distinct(selection)) %>% 
  # obtain: article, coder, matched, unmatched
  spread(matched, sel_count, fill=0) %>% # 323 rows
  # add: zero counts for article/coder where no selections made.
  ungroup() %>% bind_rows(no_selection_articles) %>% # 608 rows
  collect())

match_counts %>%
  summarize(article_count = n_distinct(article))

match_counts %>%
  group_by(article) %>%
  summarize(num_coder = n_distinct(coder)) %>%
  group_by(num_coder) %>%
  tally()

my_summarize <- function(df) {
  matched = df[[1, "matched"]]
  coder_a_name = df[[1, "coder"]]
  coder_a_only = df[[1, "unmatched"]]

  if (nrow(df) > 1) {
    coder_b_name = df[[2, "coder"]]
    coder_b_only = df[[2, "unmatched"]]
    total_selections <- matched + coder_a_only + coder_b_only
    if (total_selections == 0) {
      percent_agree <- NA
    } else {
      percent_agree <- matched / total_selections %>% round(2)
    }
  } else {
    coder_b_name <-  NA
    coder_b_only <-  NA
    total_selections <- matched + coder_a_only
    percent_agree <- NA
  }
>>>>>>> 2c405452a0bc214db2014bbb971e97da45d139cc

  tibble(percent_agree = percent_agree,
         matched = matched,
         coder_a_only = coder_a_only,
         coder_a_name = coder_a_name,
         coder_b_only = coder_b_only,
         coder_b_name = coder_b_name)
}

# Now obtain:
# article, matched, coder_a_only, coder_a_name, coder_b_only, coder_b_name 
simple_agree_counts_by_article <- match_counts %>%
  group_by(article) %>% 
  do(my_summarize(.)) %>%
  arrange(percent_agree)

simple_agree_counts_by_article %>% 
  ungroup() %>% 
  filter(!is.na(percent_agree)) %>% # single coded articles
  summarize(mean_percent_agree = mean(percent_agree))

# how many with 100%?
simple_agree_counts_by_article %>% 
  filter(!is.na(percent_agree)) %>% 
  mutate(ranking_group = case_when(
    percent_agree == 0 ~ "none",
    percent_agree <= 0.5 ~ "under_half",
    percent_agree < 1 ~ "over_half",
    percent_agree == 1 ~ "all"
    
  )) %>% 
  group_by(ranking_group) %>% 
  summarize(article_count = n(),
            group_mean = mean(percent_agree))


  
View(simple_agree_counts_by_article)
  
  #  # Now summarize by article, collapsing across coders 
  # group_by(article) %>% 
  # summarize(num_coder = n_distinct(coder),
  #           matched = sum(matched) / num_coder, # A match B, B match A
  #           unmatched = sum(unmatched)) %>% 
  # filter(num_coder > 1) %>% 
  # # for each article, summarize across columns
  # mutate(total_selections = matched + unmatched,
  #      agree_percent = ifelse(total_selections == 0, 
  #                             1.0, # neither found selections, full agree. 
  #                             (matched / total_selections) %>% round(2))) %>% 
  # arrange(desc(agree_percent)) %>% 
  # write_csv("simple_agree_counts_by_article.csv") %>% 
  # collect())



simple_agree_counts_by_article %>% group_by(num_coder) %>% tally()

simple_agree_counts_by_article %>% 
  ggplot() +
  geom_histogram(aes(x=percent_agree), bins=5)
```

```{r}
# (simple_agree_counts_by_article %>% 
#   ungroup() %>% 
#   filter(num_coders > 1) %>% 
#   summarize(total_selections = sum(total_selections),
#             total_match = sum(matched_number),
#             overall_agree = (total_match / total_selections) %>% round(2)) %>% 
#   collect())
```

```{r}
# per coder is a little more problematic; and only relevant on articles with 2
# coders.
# (agree_counts_per_coder <- found_with_matches %>% 
#   group_by(article) %>% 
#   mutate(num_coders_for_article = n_distinct(coder)) %>% 
#   filter(num_coders_for_article > 1) %>% 
#   ungroup() %>% 
#   group_by(coder, matched) %>% 
#   summarize(sel_count  = n_distinct(selection),
#             num_coders = n_distinct(coder)) %>% 
#   spread(matched, sel_count, fill=0) %>% 
#   mutate(matched_number   = (matched / 2), # adjust for A match B, B match A
#          total_selections = matched_number + unmatched,
#          agree_percent = (matched_number / total_selections) %>% round(2)) %>% 
#   collect())
# 
# View(agree_counts_per_coder)

# PMC2518490_MS01 matches PMC2518490_MD04
```

```{r}
library(tidyverse)
assignments <- read_csv("/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/data/softcite_assignments.csv")

pmc_assignments <- assignments %>% 
  filter(str_detect(pub_id, "PMC*")) %>% 
  filter(assigned == 1) %>% 
  mutate(assigned_to = str_to_lower(assigned_to))

pmc_assignees <- pmc_assignments %>% 
  pull(assigned_to) %>% unique()
  # ggplot(aes(x=asssigned_timestamp, y = assigned_to)) + 
  # geom_jitter(alpha=0.1)

pmc_assignments %>% 
  group_by(assigned_to) %>% 
  tally() %>% 
  arrange(n)

# work assigned.
assigned <- pmc_assignments %>% 
  select(article = pub_id, coder = assigned_to)

# work actually done and gathered.
completed <- match_counts %>% 
  select(article, coder)

# what was assigned but not completed? What is in pmc_assignments but not in match_counts.
missing <- anti_join(assigned, completed)

missing %>% 
  group_by(coder) %>% 
  summarize(num_missing = n(),
             missing    = str_c(article, collapse = "; ")) %>% 
  arrange(desc(num_missing)) %>% 
  filter(str_detect(coder, "carson"))
```
```{r}
# take found and nest each coder into a column?
nest_coders <- function(df) {
  coders <- split(df, df$coder)
  coders[1] <- coders[1] %>% select(-article)
  coders[2] <- coders[2] %>% select(-article)
  
  tibble(coder1 = coders[1], coder2 = coders[2])
}

(found %>% group_by(article) %>% do(nest_coders(.)))[[1,"coder1"]]
(found %>% group_by(article) %>% do(nest_coders(.)))[[10,"coder1"]]



test_ranges <- tribble(
  ~id, ~start, ~end,
  1,  1,   6,
  2,  7,   8,
  3,  1,  14,
  4,  7,  14
)



test_ranges <-  test_ranges %>% mutate(range = map2(start, end, IRanges))

ranges_array <- test_ranges %>% pull(range)

ranges_list <- as(ranges_array, "RangesList")

as.matrix(findOverlaps(query=ranges_list, select = "all", type = "any"))

range_match <- function(query, subject) {
  # match defined as any overlap.
  case_when(
    query$start > subject$end ~ FALSE,
    
  )
}

vals <- Intervals(test_ranges %>% select(start, end) %>% as.matrix)

test_ranges$matches <- interval_overlap(vals, vals)

test_ranges %>% 
  unnest() %>% 
  filter(id != matches)

```
# ~id, ~overlapping
# 1, 3
# 2, 3; 4 
# 3, 1; 2; 4
# 4, 2; 3
